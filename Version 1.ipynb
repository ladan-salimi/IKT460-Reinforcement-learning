{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7acbf555-7329-4017-b280-d26b6a6b34e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is a random question generator: the format of questions is constant. it is in form of 'Does it relate to {random.choice(self.keywords[self.category]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3ae2c5c2-5f49-4dfd-b444-1b3d8f204f3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-02 15:29:22.165320: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1740929362.185423  195445 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1740929362.191694  195445 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-03-02 15:29:22.213235: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "/home/ladans/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import random\n",
    "import json\n",
    "from transformers import TFAutoModelForCausalLM, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9f1ce593-420d-4ad8-9e37-11291d70a243",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed Keywords: {'country': ['afghanistan', 'albania', 'algeria', 'andorra', 'angola', 'antigua and barbuda', 'argentina', 'armenia', 'australia', 'austria', 'azerbaijan', 'bahrain', 'bangladesh', 'barbados', 'belarus', 'belgium', 'belize', 'benin', 'bhutan', 'bolivia', 'bosnia and herzegovina', 'botswana', 'brazil', 'brunei', 'bulgaria', 'burkina faso', 'burundi', 'cambodia', 'cameroon', 'canada', 'cape verde', 'central african republic', 'chad', 'chile', 'china', 'colombia', 'comoros', 'congo', 'costa rica', 'croatia', 'cuba', 'cyprus', 'czech republic', 'democratic republic of the congo', 'denmark', 'djibouti', 'dominica', 'dominican republic', 'ecuador', 'egypt', 'el salvador', 'england', 'equatorial guinea', 'eritrea', 'estonia', 'ethiopia', 'federated states of micronesia', 'finland', 'france', 'gabon', 'gambia', 'georgia', 'germany', 'ghana', 'greece', 'grenada', 'guatemala', 'guinea', 'guinea bissau', 'guyana', 'haiti', 'honduras', 'hungary', 'iceland', 'india', 'indonesia', 'iran', 'iraq', 'ireland', 'israel', 'italy', 'jamaica', 'japan', 'jordan', 'kazakhstan', 'kenya', 'kiribati', 'kosovo', 'kuwait', 'kyrgyzstan', 'laos', 'latvia', 'lebanon', 'lesotho', 'liberia', 'libya', 'liechtenstein', 'lithuania', 'luxembourg', 'madagascar', 'malawi', 'malaysia', 'maldives', 'mali', 'malta', 'marshall islands', 'mauritania', 'mauritius', 'mexico', 'moldova', 'monaco', 'mongolia', 'montenegro', 'morocco', 'mozambique', 'myanmar', 'namibia', 'nauru', 'nepal', 'netherlands', 'new zealand', 'nicaragua', 'niger', 'nigeria', 'north korea', 'norway', 'oman', 'pakistan', 'palau', 'palestine', 'panama', 'papua new guinea', 'paraguay', 'peru', 'philippines', 'poland', 'portugal', 'qatar', 'romania', 'russia', 'rwanda', 'saint kitts and nevis', 'saint lucia', 'saint vincent and the grenadines', 'samoa', 'san marino', 'sao tome and principe', 'saudi arabia', 'senegal', 'serbia', 'seychelles', 'sierra leone', 'singapore', 'slovakia', 'slovenia', 'solomon islands', 'somalia', 'south africa', 'south korea', 'spain', 'sudan', 'suriname', 'swaziland', 'sweden', 'switzerland', 'syria', 'taiwan', 'tajikistan', 'tanzania', 'thailand', 'togo', 'tonga', 'trinidad and tobago', 'tunisia', 'turkey', 'turkmenistan', 'tuvalu', 'uganda', 'ukraine', 'united arab emirates', 'united kingdom', 'united states of america', 'uruguay', 'uzbekistan', 'vanuatu', 'venezuela', 'vietnam', 'yemen', 'zambia', 'zimbabwe'], 'city': ['amsterdam netherlands', 'anaheim california', 'austin texas', 'auckland new zealand', 'asheville north carolina', 'ashgabat turkmenistan', 'athens greece', 'athens georgia', 'atlanta georgia', 'antwerp belgium', 'adelaide australia', 'astana kazakhstan', 'asuncion paraguay', 'algiers algeria', 'acapulco mexico', 'ankara turkey', 'baghdad iraq', 'bangkok thailand', 'beijing china', 'berlin germany', 'boston massachusetts', 'buenos aires argentina', 'bursa turkey', 'bucharest romania', 'baltimore maryland', 'beirut lebanon', 'belfast northern ireland', 'bratislava slovakia', 'belgrade serbia', 'budapest hungary', 'baku azerbaijan', 'bordeaux france', 'busan south korea', 'brussels belgium', 'bangalore india', 'calgary canada', 'chicago illinois', 'copenhagen denmark', 'columbus ohio', 'cologne germany', 'cairo egypt', 'cape town south africa', 'caracas venezuela', 'cleveland ohio', 'cork ireland', 'christchurch new zealand', 'casablanca morocco', 'chengdu china', 'cannes france', 'canberra australia', 'dallas texas', 'dubai united arab emirates', 'dhaka bangladesh', 'dakar senegal', 'delhi india', 'durban south africa', 'dublin ireland', 'dalian china', 'doha qatar', 'denver colorado', 'dusseldorf germany', 'davao city philippines', 'darwin australia', 'dunfermline scotland', 'daegu south korea', 'damascus syria', 'dar es salaam tanzania', 'edinburgh scotland', 'edmonton canada', 'essen germany', 'evora portugal', 'ensenada mexico', 'el paso texas', 'enugu nigeria', 'enschede netherlands', 'eureka california', 'erie pennsylvania', 'eilat israel', 'essentuki russia', 'esbjerg denmark', 'fez morocco', 'florence italy', 'frankfurt germany', 'fort worth texas', 'fukuoka japan', 'faisalabad pakistan', 'fujairah united arab emirates', 'funafuti tuvalu', 'florianopolis brazil', 'flinders australia', 'faro portugal', 'fujairah united arab emirates', 'fort mcmurray canada', 'fortaleza brazil', 'friesland netherlands', 'funchal portugal', 'fuzhou china', 'fresno california', 'fermoy ireland', 'fukushima japan', 'glasgow scotland', 'guangzhou china', 'gdansk poland', 'guatemala city guatemala', 'guwahati india', 'gyeongju south korea', 'genoa italy', 'grahamstown south africa', 'guadalajara mexico', 'geneva switzerland', 'graz austria', 'gwangju south korea', 'houston texas', 'hamburg germany', 'hanoi vietnam', 'helsinki finland', 'ho chi minh city vietnam', 'haifa israel', 'havana cuba', 'hong kong china', 'hobart australia', 'hangzhou china', 'hilo hawaii', 'hermosillo mexico', 'honolulu hawaii', 'helsingborg sweden', 'hiroshima japan', 'harare zimbabwe', 'istanbul turkey', 'indianapolis indiana', 'ibadan nigeria', 'istanbul turkey', 'indore india', 'izmir turkey', 'isafahan iran', 'incheon south korea', 'innsbruck austria', 'islamabad pakistan', 'ingolstadt germany', 'irvine california', 'irkutsk russia', 'jakarta indonesia', 'jerusalem israel', 'jacksonville florida', 'johannesburg south africa', 'jabalpur india', 'jinan china', 'jeddah saudi arabia', 'jalapa guatemala', 'jackson mississippi', 'juarez mexico', 'jabalpur india', 'jining china', 'kampala uganda', 'kathmandu nepal', 'kaunas lithuania', 'kuala lumpur malaysia', 'kyoto japan', 'kagoshima japan', 'karachi pakistan', 'kiev ukraine', 'kingston jamaica', 'kolkata india', 'kunming china', 'kabul afghanistan', 'kyiv ukraine', 'kawasaki japan', 'london england', 'la paz bolivia', 'los angeles california', 'lima peru', 'lyon france', 'lisbon portugal', 'luanda angola', 'liverpool england', 'lagos nigeria', 'leeds england', 'ljubljana slovenia', 'lyon france', 'lima peru', 'lviv ukraine', 'leipzig germany', 'lusaka zambia', 'lausanne switzerland', 'madrid spain', 'manchester england', 'mexico city mexico', 'manila philippines', 'montreal canada', 'milan italy', 'moscow russia', 'madrid spain', 'mumbai india', 'managua nicaragua', 'melbourne australia', 'marrakech morocco', 'miami florida', 'minneapolis minnesota', 'mecca saudi arabia', 'melbourne australia', 'makati philippines', 'monterrey mexico', 'nagoya japan', 'new york city', 'nanjing china', 'new delhi india', 'nantes france', 'noida india', 'newcastle upon tyne england', 'nice france', 'nurumberg germany', 'new orleans louisiana', 'nairobi kenya', 'naples italy', 'noosa australia', 'osaka japan', 'oklahoma city oklahoma', 'oslo norway', 'oxford england', 'ottawa canada', 'orsay france', 'odessa ukraine', 'oranjestad aruba', 'orlando florida', 'ostrava czech republic', 'oaxaca mexico', 'otago new zealand', 'ouagadougou burkina faso', 'odense denmark', 'oulu finland', 'paris france', 'prague czech republic', 'porto portugal', 'philadelphia pennsylvania', 'pyeongyang north korea', 'perth australia', 'plovdiv bulgaria', 'pattaya thailand', 'portland oregon', 'phoenix arizona', 'porto alegre brazil', 'peshawar pakistan', 'panama city panama', 'rome italy', 'rio de janeiro brazil', 'riyadh saudi arabia', 'reykjavik iceland', 'rotterdam netherlands', 'ras al khaimah united arab emirates', 'raleigh north carolina', 'riga latvia', 'rochester new york', 'recife brazil', 'san francisco california', 'sydney australia', 'singapore', 'seoul south korea', 'stockholm sweden', 'santiago chile', 'san diego california', 'shanghai china', 'sao paulo brazil', 'stuttgart germany', 'sevilla spain', 'saskatoon canada', 'san salvador el salvador', 'sofia bulgaria', 'seattle washington', 'tokyo japan', 'torino italy', 'tunis tunisia', 'tashkent uzbekistan', 'toronto canada', 'tirana albania', 'tijuana mexico', 'turin italy', 'tokyo japan', 'thessaloniki greece', 'taegu south korea', 'taksim turkey', 'taipei taiwan', 'tripoli libya', 'tokyo japan', 'ulaanbaatar mongolia', 'ubud indonesia', 'uppsala sweden', 'urumqi china', 'vaduz liechtenstein', 'vancouver canada', 'valencia spain', 'vigo spain', 'valparaiso chile', 'vladivostok russia', 'vienna austria', 'vilnius lithuania', 'villarreal spain', 'washington dc', 'westminster england', 'wilmington delaware', 'wroclaw poland', 'warsaw poland', 'wellington new zealand', 'winnipeg manitoba', 'warsaw poland', 'wuhan china', 'yokohama japan', 'york england', 'yaounde cameroon', 'yuma arizona', 'ypres belgium', 'yakutsk russia', 'yerevan armenia', 'yanbu saudi arabia', 'yogyakarta indonesia', 'yekaterinburg russia', 'zacatecas mexico', 'zunyi china', 'zincantan mexico', 'zagreb croatia', 'zeeland netherlands', 'zhongshan china', 'zanzibar tanzania', 'zurich switzerland', 'zaragoza spain'], 'landmark': ['denali', 'mount saint lias', 'mount whitney', 'mount rainier', 'iztaccihuatl', 'grand teton', 'gannett peak', 'mount adams', 'mount saint helens', 'mount shasta', 'mount saint helens', 'pikes peak', 'aconcagua', 'fitz roy', 'cotopaxi', 'chimborazo', 'mont blanc', 'zugspitze', 'mount elbrus', 'mount etna', 'everest', 'k2', 'lhotse', 'makalu', 'cho oyu', 'manaslu', 'annapurna', 'dhaulagiri', 'nanga parbat', 'kangchenjunga', 'mount fuji', 'kilimanjaro', 'meru', 'aoraki', 'haleakala', 'puncak jaya', 'sumantri', 'amazon', 'colorado river', 'dnieper', 'ganges', 'illinois river', 'mississippi river', 'nile', 'rhine', 'yangtze river', 'yellow river', 'zambezi river', 'yenisei river']}\n"
     ]
    }
   ],
   "source": [
    "# Load JSON file\n",
    "with open(\"keywords.json\", \"r\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Convert JSON list into a dictionary\n",
    "keywords = {entry[\"category\"]: [item[\"keyword\"] for item in entry[\"words\"]] for entry in data}\n",
    "\n",
    "# Print to verify structure\n",
    "print(\"Processed Keywords:\", keywords)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f86b90f2-cc5d-42dc-aa03-ea8403e4e857",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Category: landmark (Secret Word: colorado river)\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import random\n",
    "\n",
    "# Load JSON data\n",
    "with open(\"keywords.json\", \"r\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Convert JSON to a structured dictionary\n",
    "keywords = {}\n",
    "for entry in data:\n",
    "    category = entry[\"category\"]\n",
    "    words = [item[\"keyword\"] for item in entry[\"words\"]]\n",
    "    keywords[category] = words\n",
    "\n",
    "# Select a random category\n",
    "category = random.choice(list(keywords.keys()))\n",
    "\n",
    "# Select a secret word from that category\n",
    "secret_word = random.choice(keywords[category])\n",
    "\n",
    "print(f\"Category: {category} (Secret Word: {secret_word})\")\n",
    "\n",
    "# Initialize game state\n",
    "state = f\"You are playing 20 Questions. Think of a {category}.\"\n",
    "asked_questions = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4482c464-6ea7-48a1-b924-8edc1285c9dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import random\n",
    "import numpy as np\n",
    "from gym import spaces\n",
    "\n",
    "class TwentyQuestionsEnv(gym.Env):\n",
    "    def __init__(self, keywords):\n",
    "        super(TwentyQuestionsEnv, self).__init__()\n",
    "\n",
    "        # Store keywords dictionary\n",
    "        self.keywords = keywords\n",
    "\n",
    "        # Select a random category\n",
    "        self.category = random.choice(list(self.keywords.keys()))\n",
    "        self.secret_word = random.choice(self.keywords[self.category])\n",
    "\n",
    "        # Define action & observation space\n",
    "        self.action_space = spaces.Discrete(20)  # Max 20 questions\n",
    "        self.observation_space = spaces.Box(low=0, high=1, shape=(20,), dtype=np.float32)\n",
    "\n",
    "        # Initialize game state\n",
    "        self.asked_questions = []\n",
    "        self.done = False\n",
    "        self.steps = 0\n",
    "        self.max_steps = 20\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\" Takes an action (asks a question using GPT-2). \"\"\"\n",
    "        if self.done:\n",
    "            return np.array(self._get_state()), 0, True, {}\n",
    "\n",
    "        # Generate a question using random choice\n",
    "        context = \", \".join(self.keywords[self.category])  # Convert keywords to text input\n",
    "        question = f\"Does it relate to {random.choice(self.keywords[self.category])}?\"\n",
    "\n",
    "        # Simulate the answer\n",
    "        answer = self._simulate_answer(question)\n",
    "\n",
    "        # Calculate reward\n",
    "        reward = self._calculate_reward(question, answer)\n",
    "\n",
    "        # Track asked questions\n",
    "        self.asked_questions.append((question, answer))\n",
    "\n",
    "        # Check termination condition\n",
    "        self.steps += 1\n",
    "        if self.steps >= self.max_steps or answer == \"Yes, correct!\":\n",
    "            self.done = True\n",
    "\n",
    "        return np.array(self._get_state()), reward, self.done, {}\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\" Resets the environment for a new game. \"\"\"\n",
    "        self.category = random.choice(list(self.keywords.keys()))\n",
    "        self.secret_word = random.choice(self.keywords[self.category])\n",
    "        self.asked_questions = []\n",
    "        self.done = False\n",
    "        self.steps = 0\n",
    "        return np.array(self._get_state())\n",
    "\n",
    "    def _get_state(self):\n",
    "        \"\"\" Returns the current game state representation. \"\"\"\n",
    "        return [len(self.asked_questions)]  # Simplified; can be extended\n",
    "\n",
    "\n",
    "    def _simulate_answer(self, question):\n",
    "        \"\"\" Simulates answers more realistically. \"\"\"\n",
    "        if any(keyword in question.lower() for keyword in self.keywords[self.category]):\n",
    "                return random.choices([\"Yes\", \"Maybe\"], weights=[0.8, 0.2], k=1)[0]  # Positive feedback\n",
    "        return random.choices([\"No\", \"Maybe\"], weights=[0.7, 0.3], k=1)[0]  # Some randomness\n",
    "\n",
    "\n",
    "    def _calculate_reward(self, question, answer):\n",
    "        if answer == \"Yes\":\n",
    "            return 1  # Good question\n",
    "        elif answer == \"Maybe\":\n",
    "            return -0.5  # Unclear\n",
    "        elif answer == \"No\":\n",
    "            return -2  # Bad question\n",
    "        return -1 # Default penalty for irrelevant questions\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "22a53ec4-425d-4290-ac50-4d28e574f185",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "class PPOAgent:\n",
    "    def __init__(self, state_size, action_size, lr=0.001):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.lr = lr\n",
    "\n",
    "        # Build policy and value networks\n",
    "        self.policy_model = self._build_policy_model()\n",
    "        self.value_model = self._build_value_model()\n",
    "\n",
    "        # Initialize optimizers **only once**\n",
    "        self.policy_optimizer = tf.keras.optimizers.Adam(learning_rate=self.lr)\n",
    "        self.value_optimizer = tf.keras.optimizers.Adam(learning_rate=self.lr)\n",
    "\n",
    "    def _build_policy_model(self):\n",
    "        \"\"\"Defines the policy network (actor).\"\"\"\n",
    "        model = tf.keras.Sequential([\n",
    "            tf.keras.layers.Dense(64, activation='relu', input_shape=(self.state_size,)),\n",
    "            tf.keras.layers.Dense(64, activation='relu'),\n",
    "            tf.keras.layers.Dense(self.action_size, activation='softmax')\n",
    "        ])\n",
    "        return model\n",
    "\n",
    "    def _build_value_model(self):\n",
    "        \"\"\"Defines the value network (critic).\"\"\"\n",
    "        model = tf.keras.Sequential([\n",
    "            tf.keras.layers.Dense(64, activation='relu', input_shape=(self.state_size,)),\n",
    "            tf.keras.layers.Dense(64, activation='relu'),\n",
    "            tf.keras.layers.Dense(1, activation='linear')\n",
    "        ])\n",
    "        return model\n",
    "\n",
    "    def get_action(self, state):\n",
    "        \"\"\"Selects an action based on the policy network.\"\"\"\n",
    "        state = np.expand_dims(state, axis=0)  # Ensure batch dimension\n",
    "        probs = self.policy_model.predict(state, verbose=0)[0]  # Avoid TensorFlow warnings\n",
    "        return np.random.choice(self.action_size, p=probs)\n",
    "\n",
    "    def train(self, states, actions, rewards):\n",
    "        \"\"\"Updates the PPO policy using collected episode data.\"\"\"\n",
    "        states = np.array(states, dtype=np.float32)\n",
    "        actions = np.array(actions, dtype=np.int32)\n",
    "        rewards = np.array(rewards, dtype=np.float32)\n",
    "\n",
    "        # Compute discounted rewards\n",
    "        gamma = 0.99\n",
    "        discounted_rewards = np.zeros_like(rewards, dtype=np.float32)\n",
    "        cumulative = 0\n",
    "        for t in reversed(range(len(rewards))):\n",
    "            cumulative = rewards[t] + gamma * cumulative\n",
    "            discounted_rewards[t] = cumulative\n",
    "\n",
    "        # Normalize rewards\n",
    "        discounted_rewards = (discounted_rewards - np.mean(discounted_rewards)) / (np.std(discounted_rewards) + 1e-8)\n",
    "\n",
    "        # Compute advantages (A = R - V)\n",
    "        values = self.value_model.predict(states, verbose=0).flatten()\n",
    "        advantages = discounted_rewards - values\n",
    "\n",
    "        # Convert to tensors\n",
    "        states = tf.convert_to_tensor(states, dtype=tf.float32)\n",
    "        actions = tf.convert_to_tensor(actions, dtype=tf.int32)\n",
    "        advantages = tf.convert_to_tensor(advantages, dtype=tf.float32)\n",
    "        discounted_rewards = tf.convert_to_tensor(discounted_rewards, dtype=tf.float32)\n",
    "\n",
    "        # Perform training step using @tf.function to optimize execution\n",
    "        self._train_step(states, actions, advantages, discounted_rewards)\n",
    "\n",
    "    @tf.function\n",
    "    def _train_step(self, states, actions, advantages, discounted_rewards):\n",
    "        \"\"\"Performs a single PPO training step using TensorFlow functions.\"\"\"\n",
    "\n",
    "        with tf.GradientTape() as policy_tape, tf.GradientTape() as value_tape:\n",
    "            # Compute action probabilities\n",
    "            probs = self.policy_model(states, training=True)\n",
    "            action_masks = tf.one_hot(actions, self.action_size)\n",
    "            selected_probs = tf.reduce_sum(probs * action_masks, axis=1)\n",
    "\n",
    "            # Compute Policy Loss\n",
    "            policy_loss = -tf.reduce_mean(tf.math.log(selected_probs + 1e-10) * advantages)\n",
    "\n",
    "            # Compute Value Loss\n",
    "            values = self.value_model(states, training=True)\n",
    "            value_loss = tf.reduce_mean(tf.square(discounted_rewards - tf.squeeze(values)))\n",
    "\n",
    "        # Compute gradients\n",
    "        policy_grads = policy_tape.gradient(policy_loss, self.policy_model.trainable_variables)\n",
    "        value_grads = value_tape.gradient(value_loss, self.value_model.trainable_variables)\n",
    "\n",
    "        # Apply gradients\n",
    "        self.policy_optimizer.apply_gradients(zip(policy_grads, self.policy_model.trainable_variables))\n",
    "        self.value_optimizer.apply_gradients(zip(value_grads, self.value_model.trainable_variables))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "19914b07-68b4-418c-85c8-78d6f0576c44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1: Total Reward: 8.0\n",
      "Episode 2: Total Reward: 9.5\n",
      "Episode 3: Total Reward: 15.5\n",
      "Episode 4: Total Reward: 15.5\n",
      "Episode 5: Total Reward: 12.5\n",
      "Episode 6: Total Reward: 12.5\n",
      "Episode 7: Total Reward: 17.0\n",
      "Episode 8: Total Reward: 12.5\n",
      "Episode 9: Total Reward: 9.5\n",
      "Episode 10: Total Reward: 15.5\n",
      "Episode 11: Total Reward: 14.0\n",
      "Episode 12: Total Reward: 12.5\n",
      "Episode 13: Total Reward: 15.5\n",
      "Episode 14: Total Reward: 14.0\n",
      "Episode 15: Total Reward: 18.5\n",
      "Episode 16: Total Reward: 11.0\n",
      "Episode 17: Total Reward: 11.0\n",
      "Episode 18: Total Reward: 20\n",
      "Episode 19: Total Reward: 17.0\n",
      "Episode 20: Total Reward: 11.0\n",
      "Episode 21: Total Reward: 15.5\n",
      "Episode 22: Total Reward: 14.0\n",
      "Episode 23: Total Reward: 12.5\n",
      "Episode 24: Total Reward: 12.5\n",
      "Episode 25: Total Reward: 9.5\n",
      "Episode 26: Total Reward: 14.0\n",
      "Episode 27: Total Reward: 14.0\n",
      "Episode 28: Total Reward: 12.5\n",
      "Episode 29: Total Reward: 11.0\n",
      "Episode 30: Total Reward: 14.0\n",
      "Episode 31: Total Reward: 14.0\n",
      "Episode 32: Total Reward: 14.0\n",
      "Episode 33: Total Reward: 11.0\n",
      "Episode 34: Total Reward: 14.0\n",
      "Episode 35: Total Reward: 12.5\n",
      "Episode 36: Total Reward: 11.0\n",
      "Episode 37: Total Reward: 12.5\n",
      "Episode 38: Total Reward: 14.0\n",
      "Episode 39: Total Reward: 15.5\n",
      "Episode 40: Total Reward: 15.5\n",
      "Episode 41: Total Reward: 18.5\n",
      "Episode 42: Total Reward: 15.5\n",
      "Episode 43: Total Reward: 15.5\n",
      "Episode 44: Total Reward: 9.5\n",
      "Episode 45: Total Reward: 12.5\n",
      "Episode 46: Total Reward: 15.5\n",
      "Episode 47: Total Reward: 11.0\n",
      "Episode 48: Total Reward: 17.0\n",
      "Episode 49: Total Reward: 15.5\n",
      "Episode 50: Total Reward: 9.5\n",
      "Episode 51: Total Reward: 14.0\n",
      "Episode 52: Total Reward: 11.0\n",
      "Episode 53: Total Reward: 15.5\n",
      "Episode 54: Total Reward: 15.5\n",
      "Episode 55: Total Reward: 15.5\n",
      "Episode 56: Total Reward: 15.5\n",
      "Episode 57: Total Reward: 20\n",
      "Episode 58: Total Reward: 11.0\n",
      "Episode 59: Total Reward: 9.5\n",
      "Episode 60: Total Reward: 15.5\n",
      "Episode 61: Total Reward: 15.5\n",
      "Episode 62: Total Reward: 14.0\n",
      "Episode 63: Total Reward: 14.0\n",
      "Episode 64: Total Reward: 15.5\n",
      "Episode 65: Total Reward: 17.0\n",
      "Episode 66: Total Reward: 15.5\n",
      "Episode 67: Total Reward: 14.0\n",
      "Episode 68: Total Reward: 14.0\n",
      "Episode 69: Total Reward: 9.5\n",
      "Episode 70: Total Reward: 8.0\n",
      "Episode 71: Total Reward: 6.5\n",
      "Episode 72: Total Reward: 14.0\n",
      "Episode 73: Total Reward: 11.0\n",
      "Episode 74: Total Reward: 15.5\n",
      "Episode 75: Total Reward: 9.5\n",
      "Episode 76: Total Reward: 14.0\n",
      "Episode 77: Total Reward: 14.0\n",
      "Episode 78: Total Reward: 15.5\n",
      "Episode 79: Total Reward: 18.5\n",
      "Episode 80: Total Reward: 15.5\n",
      "Episode 81: Total Reward: 12.5\n",
      "Episode 82: Total Reward: 14.0\n",
      "Episode 83: Total Reward: 11.0\n",
      "Episode 84: Total Reward: 15.5\n",
      "Episode 85: Total Reward: 15.5\n",
      "Episode 86: Total Reward: 14.0\n",
      "Episode 87: Total Reward: 15.5\n",
      "Episode 88: Total Reward: 11.0\n",
      "Episode 89: Total Reward: 12.5\n",
      "Episode 90: Total Reward: 14.0\n",
      "Episode 91: Total Reward: 12.5\n",
      "Episode 92: Total Reward: 11.0\n",
      "Episode 93: Total Reward: 18.5\n",
      "Episode 94: Total Reward: 11.0\n",
      "Episode 95: Total Reward: 11.0\n",
      "Episode 96: Total Reward: 12.5\n",
      "Episode 97: Total Reward: 11.0\n",
      "Episode 98: Total Reward: 11.0\n",
      "Episode 99: Total Reward: 14.0\n",
      "Episode 100: Total Reward: 14.0\n"
     ]
    }
   ],
   "source": [
    "# Initialize environment and agent\n",
    "env = TwentyQuestionsEnv(keywords)\n",
    "state_size = 1  # Simplified; can extend\n",
    "action_size = 20\n",
    "agent = PPOAgent(state_size, action_size)\n",
    "\n",
    "episodes = 100  # Reduced for testing\n",
    "\n",
    "for episode in range(episodes):\n",
    "    state = env.reset()\n",
    "    total_reward = 0\n",
    "    rewards = []\n",
    "    states = []\n",
    "    actions = []\n",
    "\n",
    "    for t in range(env.max_steps):\n",
    "        action = agent.get_action(state)\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "\n",
    "        # Store experience\n",
    "        states.append(state)\n",
    "        actions.append(action)\n",
    "        rewards.append(reward)\n",
    "\n",
    "        state = next_state\n",
    "        total_reward += reward\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    # Update PPO policy after each episode\n",
    "    agent.train(states, actions, rewards)\n",
    "\n",
    "    print(f\"Episode {episode+1}: Total Reward: {total_reward}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8f312f9-51f5-431a-b1fb-10eab890d3ff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
