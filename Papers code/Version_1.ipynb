{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44ae8533-582d-49b2-a992-e753003625ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#fist_try_with_knowledge_using Reinforce algorithm with torch:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2b25048a-d3d8-4579-89bb-e5c223d79687",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0, Remaining Animals: 5, Reward: 1425\n",
      "Episode 50, Remaining Animals: 2, Reward: 1629\n",
      "Episode 100, Remaining Animals: 4, Reward: 1614\n",
      "Episode 150, Remaining Animals: 5, Reward: 1617\n",
      "Episode 200, Remaining Animals: 2, Reward: 1735\n",
      "Episode 250, Remaining Animals: 8, Reward: 1561\n",
      "Episode 300, Remaining Animals: 8, Reward: 1565\n",
      "Episode 350, Remaining Animals: 6, Reward: 1631\n",
      "Episode 400, Remaining Animals: 8, Reward: 1305\n",
      "Episode 450, Remaining Animals: 25, Reward: 1184\n",
      "Episode 500, Remaining Animals: 2, Reward: 1587\n",
      "Episode 550, Remaining Animals: 3, Reward: 1542\n",
      "Episode 600, Remaining Animals: 6, Reward: 1465\n",
      "Episode 650, Remaining Animals: 11, Reward: 1359\n",
      "Episode 700, Remaining Animals: 3, Reward: 1428\n",
      "Episode 750, Remaining Animals: 1, Reward: 1235\n",
      "Episode 800, Remaining Animals: 7, Reward: 1617\n",
      "Episode 850, Remaining Animals: 25, Reward: 1195\n",
      "Episode 900, Remaining Animals: 3, Reward: 1560\n",
      "Episode 950, Remaining Animals: 5, Reward: 1331\n",
      "Episode 1000, Remaining Animals: 2, Reward: 1433\n",
      "Episode 1050, Remaining Animals: 8, Reward: 1424\n",
      "Episode 1100, Remaining Animals: 5, Reward: 1530\n",
      "Episode 1150, Remaining Animals: 5, Reward: 1648\n",
      "Episode 1200, Remaining Animals: 8, Reward: 1412\n",
      "Episode 1250, Remaining Animals: 22, Reward: 1199\n",
      "Episode 1300, Remaining Animals: 10, Reward: 1352\n",
      "Episode 1350, Remaining Animals: 18, Reward: 1365\n",
      "Episode 1400, Remaining Animals: 7, Reward: 1713\n",
      "Episode 1450, Remaining Animals: 8, Reward: 1656\n",
      "Training complete!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import random\n",
    "\n",
    "# import pandas lib as pd\n",
    "import pandas as pd\n",
    "\n",
    "# read by default 1st sheet of an excel file\n",
    "df = pd.read_excel('knowledge_base1.xlsx')\n",
    "\n",
    "# Define Features\n",
    "FEATURES = [\"Hair\", \"Feathers\", \"Eggs\", \"Milk\", \"Airborne\", \"Aquatic\", \"Predator\", \"Toothed\",\n",
    "            \"Backbone\", \"Breathes\", \"Venomous\", \"Fins\", \"Nlegs_0\", \"Nlegs_2\", \"Nlegs_4\",\n",
    "            \"Nlegs_5\", \"Nlegs_6\", \"Nlegs_8\", \"Tail\", \"Domestic\", \"Catsize\",\"Mammal\",\"Bird\",\"Reptile\",\"Fish\",\"Amphibian\",\"Insect\",\"Invertebrate\"\n",
    "]\n",
    "\n",
    "NUM_ANIMALS = 100  \n",
    "NUM_FEATURES = len(FEATURES)\n",
    "MAX_STEPS = 20\n",
    "\n",
    "# Generate random animal feature matrix\n",
    "animal_features = df.iloc[:, 1:].values  # Excluding the animal names\n",
    "\n",
    "# Environment Class\n",
    "class AnimalQuestionEnv:\n",
    "    def __init__(self):\n",
    "        self.animals = list(range(NUM_ANIMALS))\n",
    "        self.target_animal = random.choice(self.animals)\n",
    "        self.remaining_animals = set(self.animals)\n",
    "        self.state = np.ones(NUM_FEATURES)\n",
    "        self.step_count = 0\n",
    "        self.done = False\n",
    "\n",
    "    def reset(self):\n",
    "        self.target_animal = random.choice(self.animals)\n",
    "        self.remaining_animals = set(self.animals)\n",
    "        self.state = np.ones(NUM_FEATURES)\n",
    "        self.step_count = 0\n",
    "        self.done = False\n",
    "        return self.state\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"Handles selecting a question (feature index).\"\"\"\n",
    "        if self.done:\n",
    "            return self.state, 0, self.done\n",
    "\n",
    "        feature_idx = int(action.item())  # Convert tensor to int\n",
    "        feature_idx = max(0, min(NUM_FEATURES - 1, feature_idx))  # Ensure valid index\n",
    "\n",
    "        correct_answer = animal_features[self.target_animal][feature_idx]\n",
    "\n",
    "        # Remove animals that do not match the answer\n",
    "        self.remaining_animals = {\n",
    "            a for a in self.remaining_animals if animal_features[a][feature_idx] == correct_answer\n",
    "        }\n",
    "\n",
    "        # Update state\n",
    "        self.state[feature_idx] = correct_answer\n",
    "        self.step_count += 1\n",
    "\n",
    "        # Reward based on elimination\n",
    "        reward = len(self.animals) - len(self.remaining_animals)\n",
    "\n",
    "        if self.step_count >= MAX_STEPS or len(self.remaining_animals) == 1:\n",
    "            self.done = True\n",
    "            guessed_animal = next(iter(self.remaining_animals), -1)\n",
    "            reward = 30 if guessed_animal == self.target_animal else -30\n",
    "\n",
    "        return self.state, reward, self.done\n",
    "\n",
    "# Policy Network\n",
    "class PolicyNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(PolicyNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(NUM_FEATURES, 64)\n",
    "        self.fc2 = nn.Linear(64, 32)\n",
    "        self.output_layer = nn.Linear(32, NUM_FEATURES)\n",
    "\n",
    "    def forward(self, state):\n",
    "        x = torch.relu(self.fc1(state))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        return torch.softmax(self.output_layer(x), dim=-1)\n",
    "\n",
    "# Training Setup\n",
    "env = AnimalQuestionEnv()\n",
    "policy_net = PolicyNetwork()\n",
    "optimizer = optim.Adam(policy_net.parameters(), lr=0.001)\n",
    "\n",
    "# Train using REINFORCE Algorithm\n",
    "gamma = 0.99\n",
    "num_episodes = 1500\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    state = env.reset()\n",
    "    log_probs = []\n",
    "    rewards = []\n",
    "\n",
    "    for t in range(MAX_STEPS):\n",
    "        state_tensor = torch.tensor(state, dtype=torch.float32).unsqueeze(0)  # Add batch dim\n",
    "        action_probs = policy_net(state_tensor)\n",
    "        action_dist = torch.distributions.Categorical(action_probs)\n",
    "        action = action_dist.sample()\n",
    "\n",
    "        log_prob = action_dist.log_prob(action)\n",
    "        log_probs.append(log_prob)\n",
    "\n",
    "        next_state, reward, done = env.step(action)\n",
    "        rewards.append(reward)\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "        state = next_state\n",
    "\n",
    "    # Compute discounted rewards\n",
    "    returns = []\n",
    "    G = 0\n",
    "    for r in reversed(rewards):#Expected rewards\n",
    "        G = r + gamma * G\n",
    "        returns.insert(0, G)\n",
    "\n",
    "    returns = torch.tensor(returns, dtype=torch.float32)\n",
    "    returns = (returns - returns.mean()) / (returns.std() + 1e-8)  # Normalize\n",
    "\n",
    "    # Compute policy loss\n",
    "    policy_loss = []\n",
    "    for log_prob, R in zip(log_probs, returns):\n",
    "        policy_loss.append(-log_prob * R)\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    sum(policy_loss).backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if episode % 50 == 0:\n",
    "        print(f\"Episode {episode}, Remaining Animals: {len(env.remaining_animals)}, Reward: {sum(rewards)}\")\n",
    "\n",
    "print(\"Training complete!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d38cb010-061c-4ee3-8718-46a62f840669",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
