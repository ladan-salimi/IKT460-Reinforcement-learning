{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "85d139fe-c9e7-4288-8ec2-7d4ec299b5c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#second_try_with_knowledge_using PPO algorithm:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d9d236e0-62f2-465b-a28c-f2fe40948ed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "import random\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, Input\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "83766102-5b05-4585-9e41-87b7812de35b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0, Remaining Animals: 8, Reward: 1543\n",
      "Episode 50, Remaining Animals: 87, Reward: 217\n",
      "Episode 100, Remaining Animals: 87, Reward: 217\n",
      "Episode 150, Remaining Animals: 87, Reward: 217\n",
      "Episode 200, Remaining Animals: 87, Reward: 217\n",
      "Episode 250, Remaining Animals: 87, Reward: 217\n",
      "Episode 300, Remaining Animals: 67, Reward: 577\n",
      "Episode 350, Remaining Animals: 4, Reward: 1652\n",
      "Episode 400, Remaining Animals: 6, Reward: 1505\n",
      "Episode 450, Remaining Animals: 2, Reward: 1600\n",
      "Training complete!\n"
     ]
    }
   ],
   "source": [
    "# read by default 1st sheet of an excel file\n",
    "df = pd.read_excel('knowledge_base1.xlsx')\n",
    "\n",
    "# Define Features\n",
    "FEATURES = [\"Hair\", \"Feathers\", \"Eggs\", \"Milk\", \"Airborne\", \"Aquatic\", \"Predator\", \"Toothed\",\n",
    "            \"Backbone\", \"Breathes\", \"Venomous\", \"Fins\", \"Nlegs_0\", \"Nlegs_2\", \"Nlegs_4\",\n",
    "            \"Nlegs_5\", \"Nlegs_6\", \"Nlegs_8\", \"Tail\", \"Domestic\", \"Catsize\",\"Mammal\",\"Bird\",\"Reptile\",\"Fish\",\"Amphibian\",\"Insect\",\"Invertebrate\"\n",
    "]\n",
    "\n",
    "animal_features = df[FEATURES].to_numpy()\n",
    "\n",
    "NUM_ANIMALS = 100  \n",
    "NUM_FEATURES = len(FEATURES)\n",
    "MAX_STEPS = 20\n",
    "\n",
    "# Environment Class\n",
    "class AnimalQuestionEnv:\n",
    "    def __init__(self):\n",
    "        self.animals = list(range(NUM_ANIMALS))\n",
    "        self.target_animal = random.choice(self.animals)\n",
    "        self.remaining_animals = set(self.animals)\n",
    "        self.state = np.ones(NUM_FEATURES)\n",
    "        self.step_count = 0\n",
    "        self.done = False\n",
    "\n",
    "    def reset(self):\n",
    "        self.target_animal = random.choice(self.animals)\n",
    "        self.remaining_animals = set(self.animals)\n",
    "        self.state = np.ones(NUM_FEATURES)\n",
    "        self.step_count = 0\n",
    "        self.done = False\n",
    "        return self.state\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"Handles selecting a question (feature index).\"\"\"\n",
    "        if self.done:\n",
    "            return self.state, 0, self.done\n",
    "\n",
    "        feature_idx = np.clip(action.numpy(), 0, NUM_FEATURES - 1)\n",
    "        correct_answer = animal_features[self.target_animal][feature_idx]\n",
    "\n",
    "        # Remove animals that do not match the answer\n",
    "        self.remaining_animals = {\n",
    "            a for a in self.remaining_animals if animal_features[a][feature_idx] == correct_answer\n",
    "        }\n",
    "\n",
    "        # Update state\n",
    "        self.state[feature_idx] = correct_answer\n",
    "        self.step_count += 1\n",
    "\n",
    "        # Reward based on elimination\n",
    "        reward = len(self.animals) - len(self.remaining_animals)\n",
    "\n",
    "        if self.step_count >= MAX_STEPS or len(self.remaining_animals) == 1:\n",
    "            self.done = True\n",
    "            guessed_animal = next(iter(self.remaining_animals), -1)\n",
    "            reward = 30 if guessed_animal == self.target_animal else -30\n",
    "\n",
    "        return self.state, reward, self.done\n",
    "\n",
    "# PPO Networks\n",
    "class PolicyNetwork(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(PolicyNetwork, self).__init__()\n",
    "        self.dense1 = tf.keras.layers.Dense(64, activation=\"relu\")\n",
    "        self.dense2 = tf.keras.layers.Dense(32, activation=\"relu\")\n",
    "        self.output_layer = tf.keras.layers.Dense(NUM_FEATURES, activation=\"softmax\")\n",
    "\n",
    "    def call(self, state):\n",
    "        x = self.dense1(state)\n",
    "        x = self.dense2(x)\n",
    "        return self.output_layer(x)\n",
    "\n",
    "class ValueNetwork(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(ValueNetwork, self).__init__()\n",
    "        self.dense1 = tf.keras.layers.Dense(64, activation=\"relu\")\n",
    "        self.dense2 = tf.keras.layers.Dense(32, activation=\"relu\")\n",
    "        self.output_layer = tf.keras.layers.Dense(1, activation=\"linear\")  # Value estimation\n",
    "\n",
    "    def call(self, state):\n",
    "        x = self.dense1(state)\n",
    "        x = self.dense2(x)\n",
    "        return self.output_layer(x)\n",
    "\n",
    "# Training Setup\n",
    "env = AnimalQuestionEnv()\n",
    "policy_net = PolicyNetwork()\n",
    "value_net = ValueNetwork()\n",
    "\n",
    "policy_optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "value_optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "\n",
    "# PPO Hyperparameters\n",
    "# PPO Hyperparameters\n",
    "gamma = 0.99\n",
    "clip_ratio = 0.2  # Clipping range for PPO updates\n",
    "num_episodes = 500\n",
    "batch_size = 32  # Number of experiences to process per update\n",
    "EPOCHS = 10  # Number of PPO training iterations per batch\n",
    "\n",
    "# Experience storage\n",
    "trajectories = []\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    state = env.reset()\n",
    "    log_probs = []\n",
    "    values = []\n",
    "    rewards = []\n",
    "    states = []\n",
    "    actions = []\n",
    "\n",
    "    for t in range(MAX_STEPS):\n",
    "        state_tensor = tf.convert_to_tensor(state, dtype=tf.float32)\n",
    "        state_tensor = tf.expand_dims(state_tensor, axis=0)\n",
    "        action_probs = policy_net(state_tensor)\n",
    "\n",
    "        # Use TensorFlow Probability for PPO action selection\n",
    "        action_distribution = tfp.distributions.Categorical(probs=action_probs)\n",
    "        action = action_distribution.sample()\n",
    "\n",
    "        log_prob = action_distribution.log_prob(action)\n",
    "        value = value_net(state_tensor)\n",
    "\n",
    "        log_probs.append(log_prob)\n",
    "        values.append(value)\n",
    "        states.append(state)\n",
    "        actions.append(action)\n",
    "\n",
    "        next_state, reward, done = env.step(action)\n",
    "        rewards.append(reward)\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "        state = next_state\n",
    "\n",
    "    # Compute discounted rewards\n",
    "    returns = []\n",
    "    G = 0\n",
    "    for r in reversed(rewards):\n",
    "        G = r + gamma * G\n",
    "        returns.insert(0, G)\n",
    "\n",
    "    returns = tf.convert_to_tensor(returns, dtype=tf.float32)\n",
    "    values = tf.convert_to_tensor(values, dtype=tf.float32)\n",
    "    log_probs = tf.convert_to_tensor(log_probs, dtype=tf.float32)\n",
    "    states = tf.convert_to_tensor(states, dtype=tf.float32)\n",
    "    actions = tf.convert_to_tensor(actions, dtype=tf.int32)\n",
    "\n",
    "    # Compute advantages\n",
    "    advantages = returns - values\n",
    "\n",
    "    # PPO Update\n",
    "    for _ in range(EPOCHS):\n",
    "        with tf.GradientTape() as policy_tape, tf.GradientTape() as value_tape:\n",
    "            new_action_probs = policy_net(states)\n",
    "            new_action_distribution = tfp.distributions.Categorical(probs=new_action_probs)\n",
    "            new_log_probs = new_action_distribution.log_prob(actions)\n",
    "\n",
    "            # PPO Clipped Surrogate Loss\n",
    "            ratio = tf.exp(new_log_probs - log_probs)\n",
    "            clipped_ratio = tf.clip_by_value(ratio, 1 - clip_ratio, 1 + clip_ratio)\n",
    "            policy_loss = -tf.reduce_mean(tf.minimum(ratio * advantages, clipped_ratio * advantages))\n",
    "\n",
    "            # Value Loss (MSE)\n",
    "            value_loss = tf.reduce_mean((returns - value_net(states)) ** 2)\n",
    "\n",
    "        # Compute gradients and apply updates\n",
    "        policy_grads = policy_tape.gradient(policy_loss, policy_net.trainable_variables)\n",
    "        value_grads = value_tape.gradient(value_loss, value_net.trainable_variables)\n",
    "\n",
    "        policy_optimizer.apply_gradients(zip(policy_grads, policy_net.trainable_variables))\n",
    "        value_optimizer.apply_gradients(zip(value_grads, value_net.trainable_variables))\n",
    "\n",
    "    if episode % 50 == 0:\n",
    "        print(f\"Episode {episode}, Remaining Animals: {len(env.remaining_animals)}, Reward: {sum(rewards)}\")\n",
    "\n",
    "print(\"Training complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d436f964-d2cc-41c4-a519-d1296727bf6e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
